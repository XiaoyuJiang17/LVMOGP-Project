{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-output 20 class classification with synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "### import Necessary packages\n",
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from models_.variational_elbo import VariationalELBO\n",
    "from models_.momc_ar_likelihood import Multi_Output_Multi_Class_AR_Likelihood\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import trange\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copy from chunchao's code...\n",
    "from sklearn.datasets import make_classification as mc\n",
    "\n",
    "X1, Y1 = mc(n_samples=2000, n_classes=20, n_features=5, n_redundant=0, n_informative=5, n_clusters_per_class=1,\n",
    "                    random_state=1)\n",
    "X = X1.copy()\n",
    "Y = Y1[:, None].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 1300\n",
    "X_train, X_test = X[:train_test_split], X[train_test_split:]\n",
    "Y_train, Y_test = Y[:train_test_split], Y[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes: 20\n",
      "The number of train data samples: 1300\n",
      "The number of test data samples: 700\n",
      "The number of features: 5\n"
     ]
    }
   ],
   "source": [
    "print('The number of classes:', Y.max() - Y.min() + 1)\n",
    "print('The number of train data samples:' , X_train.shape[0])\n",
    "print('The number of test data samples:' , X_test.shape[0])\n",
    "print('The number of features:', X_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "clf_list = [20]\n",
    "# NOTE\n",
    "# len(clf_list) the total number of outputs\n",
    "# clf_list[i] the number of classes for (i+1)th output\n",
    "n_outputs = len(clf_list)\n",
    "n_latent = int(Tensor(clf_list).sum()) # NOTE n_outputs != n_latent for general cases\n",
    "n_inputs = int(X_train.shape[0])\n",
    "index_dim = X_train.shape[-1] # this is 5\n",
    "latent_dim = 2\n",
    "n_inducing_inputs = 50\n",
    "n_inducing_latent = 5\n",
    "pca = False # Think carefully when setting this to True\n",
    "n_total= n_outputs * n_inputs\n",
    "\n",
    "n_train_iterations = 10 # 1000\n",
    "learning_rate = 0.01\n",
    "schduler_step_size = 50\n",
    "schduler_gamma = 0.8\n",
    "num_latent_MC = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = LVMOGP_SVI(n_X=n_latent, n_C=n_inputs, index_dim=index_dim, latent_dim=latent_dim, n_inducing_C=n_inducing_inputs, n_inducing_X=n_inducing_latent, data_Y=X_train.reshape(n_latent, -1), pca=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model, likelihood and training objective.\n",
    "my_model = LVMOGP_SVI(n_X=n_latent, n_C=n_inputs, index_dim=index_dim, latent_dim=latent_dim, n_inducing_C=n_inducing_inputs, n_inducing_X=n_inducing_latent, data_Y=X_train.reshape(n_latent, -1), pca=pca)\n",
    "likelihood = Multi_Output_Multi_Class_AR_Likelihood(clf_list) # how many outputs\n",
    "mll = VariationalELBO(likelihood, my_model, num_data=n_total)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.Adam([ # TODO: tune the choice of optimizer: SGD...\n",
    "    {'params': my_model.parameters()}], lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=schduler_step_size, gamma=schduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def clf_sample_f_index_everyoutput(my_model, clf_list:List, labels:Tensor, num_class_per_output=2, num_input_samples:int=3, re_index_latent_idxs=True):\n",
    "    '''\n",
    "    \n",
    "    This function returns subsampling of (all_outputs, all_classes, all_inputs) pairs.\n",
    "    All outputs are preserved, only classes and inputs are subsampled.\n",
    "    Args:\n",
    "        my_model: an instance of LVMOGP_SVI, _get_batch_idx function is in use.\n",
    "        clf_list: list of n_classes. for example, [20, 13, 17] means 3 outputs with 20, 13, 17 classes respectively.\n",
    "        labels: of shape (n_inputs, n_outputs). labels[a][b] extracts the classification label for a+1 th input at b+1 th output. \n",
    "        num_class_per_output: how many classes we want during subsampling.\n",
    "            TODO: different output has different num of classes\n",
    "        num_input_samples: how many data samples we want duing subsampling.\n",
    "\n",
    "    Return:\n",
    "        batch_index_latent: of shape (num_outputs, num_class_per_output+1, num_input_samples)\n",
    "        batch_index_inputs: of shape (num_outputs, num_class_per_output+1, num_input_samples)\n",
    "    NOTE: \n",
    "        1. Same set of inputs for every output.\n",
    "        2. Same number of classes are downsampled for every output, seems unresonable if # total classes vary a lot across outputs.\n",
    "        3. The final index on the second dim of batch_index_inputs is true label of the corresponding (input, output) pair which is useful in the future.\n",
    "    '''\n",
    "\n",
    "    num_outputs = len(clf_list)\n",
    "    input_samples = Tensor(my_model._get_batch_idx(num_input_samples, sample_X = False)).to(int)\n",
    "\n",
    "    final_inputs_idxs = input_samples.unsqueeze(0).unsqueeze(0)\n",
    "    final_inputs_idxs = final_inputs_idxs.expand(num_outputs, (num_class_per_output+1), num_input_samples)\n",
    "\n",
    "    final_latent_idxs = torch.zeros(num_outputs, (num_class_per_output+1), num_input_samples)\n",
    "\n",
    "    for i in range(num_input_samples):\n",
    "        for j in range(num_outputs):\n",
    "            curr_true_label_idx = labels[input_samples[i], j] # classification label for i+1 th input at j+1 th output ; labels[final_inputs_idxs[j,0,i]][j]\n",
    "            num_class_curr_output = clf_list[j]\n",
    "            available_range = list(np.arange(num_class_curr_output)[np.arange(num_class_curr_output) != curr_true_label_idx]) \n",
    "            assert len(available_range) == num_class_curr_output - 1\n",
    "            curr_class_idx_list = random.sample(available_range, num_class_per_output)\n",
    "            curr_class_idx_list.append(curr_true_label_idx) # of length num_class_per_output + 1\n",
    "            assert len(curr_class_idx_list) == num_class_per_output + 1\n",
    "            \n",
    "            final_latent_idxs[j,:,i] = Tensor(curr_class_idx_list)\n",
    "    \n",
    "    assert final_inputs_idxs.shape == final_latent_idxs.shape\n",
    "\n",
    "    if not re_index_latent_idxs:\n",
    "        return final_latent_idxs.to(int), final_inputs_idxs\n",
    "    \n",
    "    # Transform idx properly to better match slicing functionality from my_model.sample_latent_variable()\n",
    "    else:\n",
    "        counter = 0\n",
    "        for i in range(num_outputs):\n",
    "            final_latent_idxs[i,...] += counter\n",
    "            counter += clf_list[i]\n",
    "        return final_latent_idxs.to(int), final_inputs_idxs\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "tensor([0.4806, 0.4806, 0.4806, 0.4806, 0.4806, 0.4806, 0.4806, 0.4806, 0.4806])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/expriments/synth_expri/classification/lvmogp_s_20.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiangxiaoyu/Desktop/All%20Projects/GPLVM_project_code/expriments/synth_expri/classification/lvmogp_s_20.ipynb#X42sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(output_batch\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mreshape(batch_index_latent\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jiangxiaoyu/Desktop/All%20Projects/GPLVM_project_code/expriments/synth_expri/classification/lvmogp_s_20.ipynb#X42sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(output_batch\u001b[39m.\u001b[39mvariance\u001b[39m.\u001b[39mdetach())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jiangxiaoyu/Desktop/All%20Projects/GPLVM_project_code/expriments/synth_expri/classification/lvmogp_s_20.ipynb#X42sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(stop)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "batch_index_latent, batch_index_inputs = clf_sample_f_index_everyoutput(my_model, clf_list, Y_train)\n",
    "print(batch_index_latent)\n",
    "\n",
    "# implement 1\n",
    "imp1_start = time.time()\n",
    "sample_X_1 = my_model.sample_latent_variable()\n",
    "sample_batch_X_1 = sample_X_1[batch_index_latent]\n",
    "imp1_end = time.time()\n",
    "print('time for imp1:', imp1_end - imp1_start)\n",
    "print(sample_batch_X_1)\n",
    "# implement 2\n",
    "imp2_start = time.time()\n",
    "sample_X_2 =  my_model.sample_latent_variable(batch_index_latent)\n",
    "imp2_end = time.time()\n",
    "print('time for imp2:', imp2_end - imp2_start)\n",
    "print(sample_X_2)\n",
    "'''\n",
    "\n",
    "\n",
    "# Training!\n",
    "loss_list = []\n",
    "iterator = trange(n_train_iterations, leave=True)\n",
    "\n",
    "my_model.train()\n",
    "# likelihood.train()\n",
    "for i in iterator: \n",
    "    batch_index_latent, batch_index_inputs = clf_sample_f_index_everyoutput(my_model, clf_list, Y_train)\n",
    "    # core code is here \n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for _ in range(num_latent_MC):\n",
    "\n",
    "        sample_latent = my_model.sample_latent_variable(batch_index_latent) \n",
    "        sample_inputs = Tensor(X_train[batch_index_inputs])\n",
    "        output_batch = my_model(sample_latent.reshape(-1,latent_dim), sample_inputs.reshape(-1,index_dim)) # q(f)\n",
    "        # loss = -mll(output_batch)\n",
    "        # total_loss += loss\n",
    "    print(output_batch.loc.detach().reshape(batch_index_latent.shape))\n",
    "    print(output_batch.variance.detach())\n",
    "    print(stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training!\n",
    "loss_list = []\n",
    "iterator = trange(n_train_iterations, leave=True)\n",
    "\n",
    "my_model.train()\n",
    "likelihood.train()\n",
    "for i in iterator: \n",
    "    batch_index_latent, batch_index_inputs = clf_sample_f_index_everyoutput(my_model, clf_list, Y_train)\n",
    "    # core code is here \n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for _ in range(num_latent_MC):\n",
    "\n",
    "        sample_latent = my_model.sample_latent_variable(batch_index_latent) \n",
    "        sample_inputs = X_train[batch_index_inputs]\n",
    "        output_batch = my_model(sample_latent.reshape(-1), sample_inputs.reshape(-1)) # q(f)\n",
    "        loss = -mll(output_batch)\n",
    "        total_loss += loss\n",
    "    \n",
    "    average_loss = total_loss / num_X_MC\n",
    "    loss_list.append(average_loss.item())\n",
    "    iterator.set_description('Loss: ' + str(float(np.round(average_loss.item(),2))) + \", iter no: \" + str(i))\n",
    "    average_loss.backward()\n",
    "\n",
    "    # Gradient Clipping. Try Many Different Approaches.\n",
    "    gradient_clip(my_model, approach=gradient_clip_approach, clip_value=10)\n",
    "    gradient_clip(likelihood, clip_value=1)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.8254)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,4)\n",
    "print(a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
