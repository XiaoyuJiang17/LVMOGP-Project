{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-output 20 class classification with synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "### import Necessary packages\n",
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from models_.variational_elbo import ClfVariationalELBO\n",
    "from models_.momc_ar_likelihood import Multi_Output_Multi_Class_AR_Likelihood\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import trange\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from util_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copy from chunchao's code...\n",
    "from sklearn.datasets import make_classification as mc\n",
    "\n",
    "X1, Y1 = mc(n_samples=2000, n_classes=20, n_features=5, n_redundant=0, n_informative=5, n_clusters_per_class=1,\n",
    "                    random_state=1)\n",
    "X = X1.copy()\n",
    "Y = Y1[:, None].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 1300\n",
    "X_train, X_test = Tensor(X[:train_test_split]), Tensor(X[train_test_split:])\n",
    "Y_train, Y_test = Tensor(Y[:train_test_split]), Tensor(Y[train_test_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes: 20\n",
      "The number of train data samples: 1300\n",
      "The number of test data samples: 700\n",
      "The number of features: 5\n"
     ]
    }
   ],
   "source": [
    "print('The number of classes:', Y.max() - Y.min() + 1)\n",
    "print('The number of train data samples:' , X_train.shape[0])\n",
    "print('The number of test data samples:' , X_test.shape[0])\n",
    "print('The number of features:', X_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "clf_list = [20]\n",
    "# NOTE\n",
    "# len(clf_list) the total number of outputs\n",
    "# clf_list[i] the number of classes for (i+1)th output\n",
    "n_outputs = len(clf_list) # =1, only one output\n",
    "n_latent = int(Tensor(clf_list).sum()) # NOTE n_outputs != n_latent for general cases\n",
    "n_inputs = int(X_train.shape[0])\n",
    "index_dim = X_train.shape[-1] # this is 5\n",
    "latent_dim = 2\n",
    "n_inducing_inputs = 50\n",
    "n_inducing_latent = 5\n",
    "pca = False # Think carefully when setting this to True\n",
    "n_total= n_outputs * n_inputs\n",
    "\n",
    "n_train_iterations = 300 # 1000\n",
    "learning_rate = 0.5\n",
    "schduler_step_size = 50\n",
    "schduler_gamma = 0.9\n",
    "num_latent_MC = 3\n",
    "\n",
    "num_class_per_output = 10\n",
    "num_input_samples = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model, likelihood and training objective.\n",
    "my_model = LVMOGP_SVI(n_X=n_latent, n_C=n_inputs, index_dim=index_dim, latent_dim=latent_dim, n_inducing_C=n_inducing_inputs, n_inducing_X=n_inducing_latent, data_Y=None, pca=pca)\n",
    "likelihood = Multi_Output_Multi_Class_AR_Likelihood(clf_list)\n",
    "mll = ClfVariationalELBO(likelihood, my_model, num_data=n_total)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.Adam([ # TODO: tune the choice of optimizer: SGD...\n",
    "    {'params': my_model.parameters()}], lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=schduler_step_size, gamma=schduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4294, 0.5859],\n",
      "        [1.0281, 1.0521],\n",
      "        [2.1743, 0.6215],\n",
      "        [0.2422, 1.1671],\n",
      "        [2.6592, 2.2990],\n",
      "        [1.0740, 1.9284],\n",
      "        [1.0479, 2.8775],\n",
      "        [2.6567, 0.1208],\n",
      "        [0.7069, 0.5842],\n",
      "        [0.3092, 0.5109],\n",
      "        [0.4120, 0.2065],\n",
      "        [0.5324, 1.8765],\n",
      "        [0.6679, 0.9289],\n",
      "        [0.2445, 2.4486],\n",
      "        [0.2350, 0.3221],\n",
      "        [1.7783, 1.3141],\n",
      "        [2.2510, 1.8266],\n",
      "        [0.6575, 6.8902],\n",
      "        [0.9258, 1.6841],\n",
      "        [0.4942, 0.2156]])\n"
     ]
    }
   ],
   "source": [
    "print(my_model.X.q_log_sigma.detach().exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def clf_sample_f_index_everyoutput(my_model, clf_list:List, labels:Tensor, num_class_per_output=5, num_input_samples:int=100, re_index_latent_idxs=True):\n",
    "    '''\n",
    "    After feeding (all_outputs, all_classes, all_inputs, index_dim) of inputs, we will get f of shape (all_outputs, all_classes, all_inputs).\n",
    "    This function subsamples indices of f.\n",
    "    All outputs are preserved, only classes and inputs are subsampled.\n",
    "    Args:\n",
    "        my_model: an instance of LVMOGP_SVI, _get_batch_idx function is in use.\n",
    "        clf_list: list of n_classes. for example, [20, 13, 17] means 3 outputs with 20, 13, 17 classes respectively.\n",
    "        labels: of shape (n_inputs, n_outputs). labels[a][b] extracts the classification label for a+1 th input at b+1 th output. \n",
    "        num_class_per_output: how many classes we want during subsampling.\n",
    "            TODO: different output has different num of classes\n",
    "        num_input_samples: how many data samples we want duing subsampling.\n",
    "\n",
    "    Return:\n",
    "        batch_index_latent: of shape (num_outputs, num_class_per_output+1, num_input_samples)\n",
    "        batch_index_inputs: of shape (num_outputs, num_class_per_output+1, num_input_samples)\n",
    "    NOTE: \n",
    "        1. Same set of inputs for every output.\n",
    "        2. Same number of classes are downsampled for every output, seems unresonable if # total classes vary a lot across outputs.\n",
    "        3. The final index on the second dim of batch_index_inputs is true label of the corresponding (input, output) pair which is useful in the future.\n",
    "    '''\n",
    "\n",
    "    num_outputs = len(clf_list)\n",
    "    input_samples = Tensor(my_model._get_batch_idx(num_input_samples, sample_X = False)).to(int)\n",
    "\n",
    "    final_inputs_idxs = input_samples.unsqueeze(0).unsqueeze(0)\n",
    "    final_inputs_idxs = final_inputs_idxs.expand(num_outputs, (num_class_per_output+1), num_input_samples)\n",
    "\n",
    "    final_latent_idxs = torch.zeros(num_outputs, (num_class_per_output+1), num_input_samples)\n",
    "\n",
    "    for i in range(num_input_samples):\n",
    "        for j in range(num_outputs):\n",
    "            curr_true_label_idx = int(labels[input_samples[i], j]) # classification label for i+1 th input at j+1 th output ; labels[final_inputs_idxs[j,0,i]][j]\n",
    "            num_class_curr_output = clf_list[j]\n",
    "            available_range = list(np.arange(num_class_curr_output)[np.arange(num_class_curr_output) != curr_true_label_idx]) \n",
    "            assert len(available_range) == num_class_curr_output - 1\n",
    "            curr_class_idx_list = random.sample(available_range, num_class_per_output)\n",
    "            curr_class_idx_list.append(curr_true_label_idx) # of length num_class_per_output + 1\n",
    "            assert len(curr_class_idx_list) == num_class_per_output + 1\n",
    "            \n",
    "            final_latent_idxs[j,:,i] = Tensor(curr_class_idx_list)\n",
    "    \n",
    "    assert final_inputs_idxs.shape == final_latent_idxs.shape\n",
    "\n",
    "    if not re_index_latent_idxs:\n",
    "        return final_latent_idxs.to(int), final_inputs_idxs\n",
    "    \n",
    "    # Transform idx properly to better match slicing functionality from my_model.sample_latent_variable()\n",
    "    # as \n",
    "    else:\n",
    "        counter = 0\n",
    "        for j in range(num_outputs):\n",
    "            final_latent_idxs[j,...] += counter\n",
    "            counter += clf_list[j]\n",
    "        return final_latent_idxs.to(int), final_inputs_idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31, iter no: 299: 100%|██████████| 300/300 [03:46<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training!\n",
    "loss_list = []\n",
    "iterator = trange(n_train_iterations, leave=True)\n",
    "\n",
    "my_model.train()\n",
    "for i in iterator: \n",
    "    batch_index_latent, batch_index_inputs = clf_sample_f_index_everyoutput(my_model, clf_list, Y_train, num_class_per_output=num_class_per_output, num_input_samples=num_input_samples)\n",
    "    # core code is here \n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for _ in range(num_latent_MC):\n",
    "        sample_latent = my_model.sample_latent_variable(batch_index_latent) \n",
    "        sample_inputs = X_train[batch_index_inputs]\n",
    "        output_batch = my_model(sample_latent.reshape(-1,latent_dim), sample_inputs.reshape(-1,index_dim)) # q(f)\n",
    "        loss = -mll(output_batch, ref=batch_index_latent)\n",
    "        total_loss += loss\n",
    "    \n",
    "    average_loss = total_loss / num_latent_MC\n",
    "    loss_list.append(average_loss.item())\n",
    "    iterator.set_description('Loss: ' + str(float(np.round(average_loss.item(),2))) + \", iter no: \" + str(i))\n",
    "    average_loss.backward()\n",
    "    \n",
    "    # Gradient Clipping. Try Many Different Approaches.\n",
    "    gradient_clip(my_model, approach='Global Norm Clipping', clip_value=5)\n",
    "    # gradient_clip(likelihood, clip_value=1)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f93dba84ee0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model.X.q_mu.detach()\n",
    "# my_model.X.q_log_sigma.detach().exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax_function(f_mean:Tensor, f_var:Tensor, num_MC_samples:int=10):\n",
    "    '''\n",
    "    Single output softmax funciton, i.e., given latent parameter values, we get probabilities for each class.\n",
    "    The reparametrization trick is in use for Monte Carlo estimation ... \n",
    "    The methodology is from paper:\n",
    "        <Scalable Gaussian Process Classification With Additive Noise for Non-Gaussian Likelihoods> 2022, Liu et al.\n",
    "    \n",
    "    Args:\n",
    "        f_mean: of shape (n_test_samples, n_classes)\n",
    "        f_var: of shape (n_test_samples, n_classes)\n",
    "    \n",
    "    Return:\n",
    "        results_prob_mean: of shape (n_test_samples, n_classes)\n",
    "        results_prob_var: of shape (n_test_samples, n_classes)\n",
    "        results_decisions: of shape (n_test_samples)\n",
    "    '''\n",
    "    n_test_samples, n_classes = f_mean.shape[0], f_mean.shape[1]\n",
    "    # reparametrization trick for MC estimation!\n",
    "    sample_f = f_mean.unsqueeze(-1).expand(-1, -1, num_MC_samples) + f_var.sqrt().unsqueeze(-1).expand(-1, -1, num_MC_samples) + torch.randn(n_test_samples, n_classes, num_MC_samples)\n",
    "    assert sample_f.shape == torch.Size([n_test_samples, n_classes, num_MC_samples])\n",
    "    exp_sample_f_term = sample_f.exp()\n",
    "    exp_sample_f_sum_term = exp_sample_f_term.sum(1).unsqueeze(1).expand(-1, n_classes, -1) # sum over n_classes, then expand to proper size (for future use)\n",
    "    softmax_ratios = exp_sample_f_term / exp_sample_f_sum_term\n",
    "\n",
    "    results_prob_mean = softmax_ratios.sum(-1)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOMOC_predict(my_model, X_test:Tensor, clf_list:List, test_mini_batch:int =200, mode='All_Outputs'):\n",
    "    '''\n",
    "    Args:\n",
    "        my_model: trained model.\n",
    "        X_test: test input locations. of shape (num_test_inputs, num_features).\n",
    "        clf_list: list of # of classes (for every output). \n",
    "        mode: whether all output predictions of all test inputs are needed.\n",
    "    Return:\n",
    "        classification_results: dict, ('mean', i, j) is an example key for the mean vector (softmax-generated) of jth output ith test example. \n",
    "        classification_decision: tensor of shape (num_test_inputs, num_outputs)\n",
    "    '''\n",
    "    my_model.eval()\n",
    "    n_outputs = len(clf_list)\n",
    "    n_test_samples = X_test.shape[0]\n",
    "    X_q_mu = my_model.X.q_mu.detach()\n",
    "    n_latent = X_q_mu.shape[0]\n",
    "\n",
    "    if mode == 'All_Outputs':\n",
    "\n",
    "        # NOTE we would like two equal length 1d tensor for extracting elements in X_q_mu and X_test and feed them to my_model.\n",
    "        # the length is n_latent * n_test_samples\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # * we will get prediction results input by input. i.e. the first batch of outputs (of length n_latent) are for first test input, followed by\n",
    "        # second input, third input and so on. \n",
    "\n",
    "        # * for first n_latent of prediction outputs, they are ordered task by task. i.e. first batch of them (of length clf_list[0]) are for first task,\n",
    "        # followed by second task (of first input) and so on.\n",
    "\n",
    "        # * by doing this, we may have very very long tensor which might not available for feeding into the model entirely at once. the solution here is\n",
    "        # applying mini-batching ...\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        all_latent_index = Tensor(np.arange(n_latent)).repeat(n_test_samples)\n",
    "        all_input_index = Tensor([i for i in range(n_test_samples) for _ in range(n_latent)])\n",
    "\n",
    "        assert all_input_index.shape == all_latent_index.shape\n",
    "        assert all_latent_index[:n_latent].var != 0.0\n",
    "        assert all_input_index[:n_latent].var == 0.0 # all same elements\n",
    "\n",
    "        test_mini_batch = test_mini_batch\n",
    "        pred_results_mean = []\n",
    "        pred_results_var = []\n",
    "        test_continue = True\n",
    "        start_idx = 0\n",
    "        end_idx = test_mini_batch\n",
    "        while test_continue:\n",
    "            batch_latent = X_q_mu[all_latent_index[start_idx:end_idx]] # TODO: only mean are taken into consideration ...\n",
    "            batch_inputs = X_test[all_input_index[start_idx:end_idx]]\n",
    "            batch_output = my_model(batch_latent, batch_inputs) # q(f): batch prediction\n",
    "            pred_results_mean.append(batch_output.loc.detach().tolist())\n",
    "            pred_results_var.append(batch_output.variance.detach().tolist())\n",
    "\n",
    "            if start_idx < n_latent * n_test_samples:\n",
    "                start_idx += test_mini_batch\n",
    "                end_idx += test_mini_batch\n",
    "            else:\n",
    "                test_continue = False\n",
    "\n",
    "        assert len(pred_results_mean) == len(pred_results_var) == int(n_latent * n_test_samples)\n",
    "        \n",
    "        pred_results_mean_tensor = Tensor(pred_results_mean).reshape(n_test_samples, n_latent)\n",
    "        pred_results_var_tensor  = Tensor(pred_results_var).reshape(n_test_samples, n_latent)\n",
    "\n",
    "        # NOTE: n_latent is the number of all latents for all outputs.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # \n",
    "\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor name, param in my_model.named_parameters():\\n    if 'variational_strategy.inducing' in name:\\n        print(name, param.size())\\n        print(param)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 'variational_strategy.inducing' in name:\n",
    "        print(name, param.size())\n",
    "        print(param)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape == torch.Size([700, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1595, -0.5365],\n",
      "        [ 0.9341, -2.3207],\n",
      "        [ 0.4767, -0.1484]])\n",
      "tensor([1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,2)\n",
    "print(a)\n",
    "print(a.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
