{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-output 20 class classification with synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "### import Necessary packages\n",
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from models_.variational_elbo import ClfVariationalELBO\n",
    "from models_.momc_ar_likelihood import Multi_Output_Multi_Class_AR_Likelihood\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR\n",
    "from tqdm import trange\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from util_functions import *\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copy from chunchao's code...\n",
    "from sklearn.datasets import make_classification as mc\n",
    "\n",
    "X1, Y1 = mc(n_samples=2000, n_classes=20, n_features=5, n_redundant=0, n_informative=5, n_clusters_per_class=1,\n",
    "                    random_state=1)\n",
    "X = X1.copy()\n",
    "Y = Y1[:, None].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 1300\n",
    "X_train, X_test = Tensor(X[:train_test_split]), Tensor(X[train_test_split:])\n",
    "Y_train, Y_test = Tensor(Y[:train_test_split]), Tensor(Y[train_test_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes: 20\n",
      "The number of train data samples: 1300\n",
      "The number of test data samples: 700\n",
      "The number of features: 5\n"
     ]
    }
   ],
   "source": [
    "print('The number of classes:', Y.max() - Y.min() + 1)\n",
    "print('The number of train data samples:' , X_train.shape[0])\n",
    "print('The number of test data samples:' , X_test.shape[0])\n",
    "print('The number of features:', X_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "clf_list = [20]\n",
    "# NOTE\n",
    "# len(clf_list) the total number of outputs\n",
    "# clf_list[i] the number of classes for (i+1)th output\n",
    "n_outputs = len(clf_list) # =1, only one output\n",
    "n_latent = int(Tensor(clf_list).sum()) # NOTE n_outputs != n_latent for general cases\n",
    "n_inputs = int(X_train.shape[0])\n",
    "index_dim = X_train.shape[-1] # this is 5\n",
    "latent_dim = 2\n",
    "n_inducing_inputs = 10\n",
    "n_inducing_latent = 3\n",
    "pca = False # Think carefully when setting this to True\n",
    "n_total= n_outputs * n_inputs\n",
    "\n",
    "n_train_iterations = 300 # 1000\n",
    "learning_rate = 0.5\n",
    "schduler_step_size = 50\n",
    "schduler_gamma = 0.9\n",
    "num_latent_MC = 3\n",
    "\n",
    "num_class_per_output = 10\n",
    "num_input_samples = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model, likelihood and training objective.\n",
    "my_model = LVMOGP_SVI(n_X=n_latent, n_C=n_inputs, index_dim=index_dim, latent_dim=latent_dim, n_inducing_C=n_inducing_inputs, n_inducing_X=n_inducing_latent, data_Y=None, pca=pca)\n",
    "likelihood = Multi_Output_Multi_Class_AR_Likelihood(clf_list)\n",
    "mll = ClfVariationalELBO(likelihood, my_model, num_data=n_total)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.Adam([ # TODO: tune the choice of optimizer: SGD...\n",
    "    {'params': my_model.parameters()}], lr=learning_rate)\n",
    "\n",
    "# scheduler = CyclicLR(optimizer, base_lr=0.01, max_lr=0.5, step_size_up=20, mode='triangular', cycle_momentum=False)\n",
    "scheduler = StepLR(optimizer, step_size=schduler_step_size, gamma=schduler_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(my_model.X.q_log_sigma.detach().exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_sample_f_index_everyoutput(my_model, clf_list:List, labels:Tensor, num_class_per_output=5, num_input_samples:int=100, re_index_latent_idxs=True):\n",
    "    '''\n",
    "    After feeding (all_outputs, all_classes, all_inputs, index_dim) of inputs, we will get f of shape (all_outputs, all_classes, all_inputs).\n",
    "    This function subsamples indices of f.\n",
    "    All outputs are preserved, only classes and inputs are subsampled.\n",
    "    Args:\n",
    "        my_model: an instance of LVMOGP_SVI, _get_batch_idx function is in use.\n",
    "        clf_list: list of n_classes. for example, [20, 13, 17] means 3 outputs with 20, 13, 17 classes respectively.\n",
    "        labels: of shape (n_inputs, n_outputs). labels[a][b] extracts the classification label for a+1 th input at b+1 th output. \n",
    "        num_class_per_output: how many classes we want during subsampling.\n",
    "            TODO: different output has different num of classes\n",
    "        num_input_samples: how many data samples we want duing subsampling.\n",
    "\n",
    "    Return:\n",
    "        batch_index_latent: of shape (num_outputs, num_class_per_output+1, num_input_samples)\n",
    "        batch_index_inputs: of shape (num_outputs, num_class_per_output+1, num_input_samples)\n",
    "    NOTE: \n",
    "        1. Same set of inputs for every output.\n",
    "        2. Same number of classes are downsampled for every output, seems unresonable if # total classes vary a lot across outputs.\n",
    "        3. The final index on the second dim of batch_index_inputs is true label of the corresponding (input, output) pair which is useful in the future.\n",
    "    '''\n",
    "\n",
    "    num_outputs = len(clf_list)\n",
    "    input_samples = Tensor(my_model._get_batch_idx(num_input_samples, sample_X = False)).to(int)\n",
    "\n",
    "    final_inputs_idxs = input_samples.unsqueeze(0).unsqueeze(0)\n",
    "    final_inputs_idxs = final_inputs_idxs.expand(num_outputs, (num_class_per_output+1), num_input_samples)\n",
    "\n",
    "    final_latent_idxs = torch.zeros(num_outputs, (num_class_per_output+1), num_input_samples)\n",
    "\n",
    "    for i in range(num_input_samples):\n",
    "        for j in range(num_outputs):\n",
    "            curr_true_label_idx = int(labels[input_samples[i], j]) # classification label for i+1 th input at j+1 th output ; labels[final_inputs_idxs[j,0,i]][j]\n",
    "            num_class_curr_output = clf_list[j]\n",
    "            available_range = list(np.arange(num_class_curr_output)[np.arange(num_class_curr_output) != curr_true_label_idx]) \n",
    "            assert len(available_range) == num_class_curr_output - 1\n",
    "            curr_class_idx_list = random.sample(available_range, num_class_per_output)\n",
    "            curr_class_idx_list.append(curr_true_label_idx) # of length num_class_per_output + 1\n",
    "            assert len(curr_class_idx_list) == num_class_per_output + 1\n",
    "            \n",
    "            final_latent_idxs[j,:,i] = Tensor(curr_class_idx_list)\n",
    "    \n",
    "    assert final_inputs_idxs.shape == final_latent_idxs.shape\n",
    "\n",
    "    if not re_index_latent_idxs:\n",
    "        return final_latent_idxs.to(int), final_inputs_idxs\n",
    "    \n",
    "    # Transform idx properly to better match slicing functionality from my_model.sample_latent_variable()\n",
    "    # as \n",
    "    else:\n",
    "        counter = 0\n",
    "        for j in range(num_outputs):\n",
    "            final_latent_idxs[j,...] += counter\n",
    "            counter += clf_list[j]\n",
    "        return final_latent_idxs.to(int), final_inputs_idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31, iter no: 299: 100%|██████████| 300/300 [01:54<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training!\n",
    "loss_list = []\n",
    "iterator = trange(n_train_iterations, leave=True)\n",
    "\n",
    "my_model.train()\n",
    "for i in iterator: \n",
    "    batch_index_latent, batch_index_inputs = clf_sample_f_index_everyoutput(my_model, clf_list, Y_train, num_class_per_output=num_class_per_output, num_input_samples=num_input_samples)\n",
    "    # core code is here \n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    for _ in range(num_latent_MC):\n",
    "        sample_latent = my_model.sample_latent_variable(batch_index_latent) \n",
    "        sample_inputs = X_train[batch_index_inputs]\n",
    "        output_batch = my_model(sample_latent.reshape(-1,latent_dim), sample_inputs.reshape(-1,index_dim)) # q(f)\n",
    "        loss = -mll(output_batch, ref=batch_index_latent)\n",
    "        total_loss += loss\n",
    "    \n",
    "    average_loss = total_loss / num_latent_MC\n",
    "    loss_list.append(average_loss.item())\n",
    "    iterator.set_description('Loss: ' + str(float(np.round(average_loss.item(),2))) + \", iter no: \" + str(i))\n",
    "    average_loss.backward()\n",
    "    \n",
    "    # Gradient Clipping. Try Many Different Approaches.\n",
    "    gradient_clip(my_model, approach='Global Norm Clipping', clip_value=5)\n",
    "    # gradient_clip(likelihood, clip_value=1)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc1ef81ce20>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model.X.q_mu.detach()\n",
    "# my_model.X.q_log_sigma.detach().exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax_function(f_mean:Tensor, f_var:Tensor, num_MC_samples:int=50):\n",
    "    '''\n",
    "    Single output softmax funciton, i.e., given latent parameter values, we get probabilities for each class.\n",
    "    The reparametrization trick is in use for Monte Carlo estimation ... \n",
    "    The methodology is from paper:\n",
    "        <Scalable Gaussian Process Classification With Additive Noise for Non-Gaussian Likelihoods> 2022, Liu et al.\n",
    "    \n",
    "    Args:\n",
    "        f_mean: of shape (n_test_samples, n_classes)\n",
    "        f_var: of shape (n_test_samples, n_classes)\n",
    "    \n",
    "    Return:\n",
    "        results_prob_mean: of shape (n_test_samples, n_classes)\n",
    "        results_prob_var: of shape (n_test_samples, n_classes)\n",
    "        results_decisions: of shape (n_test_samples)\n",
    "        results_decisions_var: of shape (n_test_samples)\n",
    "    '''\n",
    "    n_test_samples, n_classes = f_mean.shape[0], f_mean.shape[1]\n",
    "    # reparametrization trick for MC estimation!\n",
    "    sample_f = f_mean.unsqueeze(-1).expand(-1, -1, num_MC_samples) + f_var.sqrt().unsqueeze(-1).expand(-1, -1, num_MC_samples) + torch.randn(n_test_samples, n_classes, num_MC_samples)\n",
    "    assert sample_f.shape == torch.Size([n_test_samples, n_classes, num_MC_samples])\n",
    "    exp_sample_f_term = sample_f.exp()\n",
    "    exp_sample_f_sum_term = exp_sample_f_term.sum(1).unsqueeze(1).expand(-1, n_classes, -1) # sum over n_classes, then expand to proper size (for future use)\n",
    "    softmax_ratios = exp_sample_f_term / exp_sample_f_sum_term\n",
    "\n",
    "    results_prob_mean = softmax_ratios.mean(-1)\n",
    "    results_prob_var = softmax_ratios.var(-1)\n",
    "    results_decisions = results_prob_mean.argmax(1)\n",
    "    results_decisions_var = results_prob_var.sum(1)\n",
    "\n",
    "    return results_prob_mean, results_prob_var, results_decisions, results_decisions_var  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOMOC_predict(my_model, X_test:Tensor, clf_list:List, test_mini_batch:int=201, num_MC_samples:int=50, mode='All_Outputs'):\n",
    "    '''\n",
    "    MultiOutput MultiClass prediction given:\n",
    "    Args:\n",
    "        my_model: trained model.\n",
    "        X_test: test input locations. of shape (n_test_samples, num_features).\n",
    "        clf_list: list of # of classes (for every output). \n",
    "        mode: whether all output predictions of all test inputs are needed.\n",
    "    Return:\n",
    "        all_outputs_prob_mean, all_outputs_prob_var: List of tensors. length of List is n_outputs, shape of each tensor is (num_test_inputs, num_classes), num_classes might varies for different output.\n",
    "        all_outputs_decisions, all_outputs_decisions_var: tensor of shape (n_test_samples, num_outputs)\n",
    "    '''\n",
    "    my_model.eval()\n",
    "    n_outputs = len(clf_list)\n",
    "    n_test_samples = X_test.shape[0]\n",
    "    X_q_mu = my_model.X.q_mu.detach()\n",
    "    n_latent = X_q_mu.shape[0]\n",
    "\n",
    "    if mode == 'All_Outputs':\n",
    "\n",
    "        # NOTE we would like two equal length 1d tensor for extracting elements in X_q_mu and X_test and feed them to my_model.\n",
    "        # the length is n_latent * n_test_samples\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # * we will get prediction results input by input. i.e. the first batch of outputs (of length n_latent) are for first test input, followed by\n",
    "        # second input, third input and so on. \n",
    "\n",
    "        # * for first n_latent of prediction outputs, they are ordered task by task. i.e. first batch of them (of length clf_list[0]) are for first task,\n",
    "        # followed by second task (of first input) and so on.\n",
    "\n",
    "        # * by doing this, we may have very very long tensor which might not available for feeding into the model entirely at once. the solution here is\n",
    "        # applying mini-batching ...\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        all_latent_index = Tensor(np.arange(n_latent)).repeat(n_test_samples)\n",
    "        all_input_index = Tensor([i for i in range(n_test_samples) for _ in range(n_latent)])\n",
    "\n",
    "        assert all_input_index.shape == all_latent_index.shape\n",
    "        assert (all_latent_index[:n_latent]).var() != 0.0\n",
    "        assert (all_input_index[:n_latent]).var() == 0.0 # all same elements\n",
    "\n",
    "        test_mini_batch = test_mini_batch\n",
    "        pred_results_mean = torch.zeros(int(n_latent * n_test_samples))\n",
    "        pred_results_var = torch.zeros(int(n_latent * n_test_samples))\n",
    "        test_continue = True\n",
    "        start_idx = 0\n",
    "        end_idx = test_mini_batch\n",
    "        while test_continue:\n",
    "            batch_latent = X_q_mu[all_latent_index[start_idx:end_idx].to(int)] # TODO: only mean are taken into consideration ...\n",
    "            batch_inputs = X_test[all_input_index[start_idx:end_idx].to(int)]\n",
    "            batch_output = my_model(batch_latent, batch_inputs) # q(f): batch prediction\n",
    "            pred_results_mean[start_idx:end_idx] = batch_output.loc.detach()\n",
    "            pred_results_var[start_idx:end_idx] = batch_output.variance.detach()\n",
    "            # pred_results_mean.append(batch_output.loc.detach().tolist()) # This will leads to list of list, which is not desirable...\n",
    "            # pred_results_var.append(batch_output.variance.detach().tolist())\n",
    "\n",
    "            if end_idx < n_latent * n_test_samples:\n",
    "                start_idx += test_mini_batch\n",
    "                end_idx += test_mini_batch\n",
    "                end_idx = min(end_idx, int(n_latent * n_test_samples))\n",
    "            else:\n",
    "                test_continue = False\n",
    "\n",
    "        assert len(pred_results_mean) == len(pred_results_var) == int(n_latent * n_test_samples)\n",
    "        \n",
    "        pred_results_mean_tensor = Tensor(pred_results_mean).reshape(n_test_samples, n_latent)\n",
    "        pred_results_var_tensor  = Tensor(pred_results_var).reshape(n_test_samples, n_latent)\n",
    "        \n",
    "        # NOTE: n_latent is the number of all latents for all outputs.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        # We need to chunk pred_results_mean_tensor and pred_results_var_tensor into n_outputs tensors, each of them can be fed into previously defined\n",
    "        # Softmax_function for getting predictions.\n",
    "        # ------------------------------------------------------------------------------------------------------------------------------\n",
    "        split_mean_tensors = torch.split(pred_results_mean_tensor, clf_list, dim=-1) # tuple of tensors\n",
    "        split_var_tensors = torch.split(pred_results_var_tensor, clf_list, dim=-1)\n",
    "        assert n_outputs == len(split_mean_tensors) == len(split_var_tensors)\n",
    "\n",
    "        all_outputs_prob_mean = []\n",
    "        all_outputs_prob_var = []\n",
    "        all_outputs_decisions = torch.zeros(n_test_samples, n_outputs)\n",
    "        all_outputs_decisions_var = torch.zeros(n_test_samples, n_outputs)\n",
    "\n",
    "\n",
    "        for i in range(n_outputs):\n",
    "            _prob_mean, _prob_var, _decisions, _decisions_var = Softmax_function(f_mean=split_mean_tensors[i], f_var=split_var_tensors[i], num_MC_samples=num_MC_samples)\n",
    "            all_outputs_prob_mean.append(_prob_mean) # List of tensors\n",
    "            all_outputs_prob_var.append(_prob_var)   # List of tensors\n",
    "            all_outputs_decisions[:,i] = _decisions\n",
    "            all_outputs_decisions_var[:,i] = _decisions_var\n",
    "\n",
    "    return all_outputs_prob_mean, all_outputs_prob_var, all_outputs_decisions, all_outputs_decisions_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOMC_classification_eval(predictions:Tensor, labels:Tensor) -> Dict[str, List[float]]:\n",
    "    '''\n",
    "    This function evaluate classification performance for every output given predictions tensor and labels tensor.\n",
    "    \n",
    "    Evaluation metrices are: precisions weighted, recall weighted and F1 weighted.\n",
    "\n",
    "    Args:\n",
    "        predictions: Tensor of shape (num_samples, num_outputs)\n",
    "        labels: Tensor of shape (num_samples, num_outputs)\n",
    "    \n",
    "    Return:\n",
    "        eval_results: dictionary, keys: Precision_Weighted, Recall_Weighted, F1_weighted.\n",
    "                    for each key, the dict contains a list of performance for all outputs.\n",
    "    '''\n",
    "    eval_results = {'Precision_Weighted': [], 'Recall_Weighted': [], 'F1_Weighted': []}  \n",
    "\n",
    "    # Iterate through each output\n",
    "    for output in range(labels.size(1)):\n",
    "        # Extract predictions and labels for the current output\n",
    "        preds = predictions[:, output]\n",
    "        lbls = labels[:, output]\n",
    "\n",
    "        # Calculate weighted metrics for the current output\n",
    "        precision = precision_score(lbls, preds, average='weighted')\n",
    "        recall = recall_score(lbls, preds, average='weighted')\n",
    "        f1 = f1_score(lbls, preds, average='weighted')\n",
    "\n",
    "        # Store the results\n",
    "        eval_results['Precision_Weighted'].append(precision)\n",
    "        eval_results['Recall_Weighted'].append(recall)\n",
    "        eval_results['F1_Weighted'].append(f1)\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor name, param in my_model.named_parameters():\\n    if 'variational_strategy.inducing' in name:\\n        print(name, param.size())\\n        print(param)\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for name, param in my_model.named_parameters():\n",
    "    if 'variational_strategy.inducing' in name:\n",
    "        print(name, param.size())\n",
    "        print(param)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs_prob_mean, all_outputs_prob_var, all_outputs_decisions, all_outputs_decisions_var = MOMOC_predict(my_model, X_train, clf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0585)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0 * (all_outputs_decisions == Y_train)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = MOMC_classification_eval(all_outputs_decisions, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision_Weighted': [0.05823393716057524],\n",
       " 'Recall_Weighted': [0.05846153846153846],\n",
       " 'F1_Weighted': [0.0578988121943782]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
