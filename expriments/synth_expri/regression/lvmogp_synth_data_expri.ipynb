{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from linear_operator.operators import KroneckerProductLinearOperator\n",
    "from torch import Tensor\n",
    "from torch.distributions import MultivariateNormal\n",
    "from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from models_.gaussian_likelihood import GaussianLikelihood\n",
    "from models_.variational_elbo import VariationalELBO\n",
    "from tqdm import trange\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from util_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Without missing data \n",
    "#### Note we keep any C_index-like variable referring to indices in total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expri_random_seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in synthetic data (only call this function once)\n",
    "wo_n_C_total = 700 # totally 700 points for C\n",
    "wo_n_outputs = 20\n",
    "wo_X_true, wo_C_total, wo_sample_total_data, kernel_parameters = tidily_sythetic_data_from_MOGP(n_C=wo_n_C_total, n_X=wo_n_outputs, random_seed=expri_random_seed)\n",
    "# sample_total_data: of length n_outputs * n_C_total.\n",
    "\n",
    "wo_n_C_train = 20\n",
    "wo_n_C_test = wo_n_C_total - wo_n_C_train\n",
    "\n",
    "random.seed(expri_random_seed)\n",
    "wo_train_C_tidily_indices = random.sample(range(wo_n_C_total), wo_n_C_train) # of length n_C_train\n",
    "wo_test_C_tidily_indices = [index for index in range(wo_n_C_total) if index not in wo_train_C_tidily_indices] # of length n_C_test\n",
    "\n",
    "# list (n_outputs) of list (n_C_train), \n",
    "wo_ls_of_ls_train_C = [wo_train_C_tidily_indices for _ in range(wo_n_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_sample_train_index = wo_train_C_tidily_indices\n",
    "for i in range(1,wo_n_outputs): # 20 outputs, except the first one is already included\n",
    "    wo_sample_train_index = np.concatenate((wo_sample_train_index, list(np.array(wo_train_C_tidily_indices) + wo_n_C_total*i)))\n",
    "\n",
    "wo_sample_test_index = wo_test_C_tidily_indices\n",
    "for i in range(1,wo_n_outputs): # 20 outputs, except the first one is already included\n",
    "    wo_sample_test_index = np.concatenate((wo_sample_test_index, list(np.array(wo_test_C_tidily_indices) + wo_n_C_total*i)))\n",
    "\n",
    "assert wo_sample_train_index.shape[0] == wo_n_C_train * wo_n_outputs\n",
    "assert wo_sample_test_index.shape[0] == wo_n_C_test* wo_n_outputs\n",
    "assert np.isin(wo_sample_train_index, wo_sample_test_index).sum() == 0\n",
    "\n",
    "wo_sample_train_data = wo_sample_total_data[wo_sample_train_index]\n",
    "wo_sample_test_data = wo_sample_total_data[wo_sample_test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Variables here are: ls_of_ls_train_C, sample_train_index, sample_test_index, sample_train_data, sample_test_data, sample_total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define hyper-parameters\n",
    "train_style = 'all-together' # 'all-together' or 'em-style'\n",
    "train_parameters =  'all-parameters' #'fix-kernel-parameters' or 'all-parameters'\n",
    "wo_n_X_model = wo_n_outputs\n",
    "wo_n_C_model = wo_n_C_train\n",
    "wo_n_total_model = wo_n_X_model * wo_n_C_model\n",
    "wo_index_dim = 1\n",
    "wo_latent_dim = 2\n",
    "wo_n_inducing_C = 15\n",
    "wo_n_inducing_X = 30\n",
    "wo_pca = False\n",
    "\n",
    "# specify model\n",
    "wo_my_model = LVMOGP_SVI(wo_n_X_model, wo_n_C_model, wo_index_dim, wo_latent_dim, wo_n_inducing_C, wo_n_inducing_X, wo_sample_train_data.reshape(wo_n_X_model, -1), pca=wo_pca)\n",
    "\n",
    "# Likelihood & training objective\n",
    "wo_likelihood = GaussianLikelihood()\n",
    "wo_mll = VariationalELBO(wo_likelihood, wo_my_model, num_data=wo_n_total_model)\n",
    "\n",
    "# have a look at parameters\n",
    "for name, param in wo_my_model.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "print('---' * 15) \n",
    "for name, param in wo_likelihood.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# Initialize inducing points in C space\n",
    "wo_my_model.variational_strategy.inducing_points_C.data = torch.rand(wo_n_inducing_C).reshape(-1,1) * 20 - 10\n",
    "# This depends on interval (-10,10) appear in tidily_sythetic_data_from_MOGP\n",
    "\n",
    "# following parameters are initialized as ground truth values ... and fixed!\n",
    "if train_parameters == 'fix-kernel-parameters':\n",
    "    wo_my_model.covar_module_X.raw_outputscale.data = kernel_parameters['X_raw_outputscale']\n",
    "    wo_my_model.covar_module_X.raw_outputscale.requires_grad = False\n",
    "    wo_my_model.covar_module_X.base_kernel.raw_lengthscale.data = kernel_parameters['X_raw_lengthscale']\n",
    "    wo_my_model.covar_module_X.base_kernel.raw_lengthscale.requires_grad = False\n",
    "    wo_my_model.covar_module_C.raw_outputscale.data = kernel_parameters['C_raw_outputscale']\n",
    "    wo_my_model.covar_module_C.raw_outputscale.requires_grad = False\n",
    "    wo_my_model.covar_module_C.base_kernel.raw_lengthscale.data = kernel_parameters['C_raw_lengthscale']\n",
    "    wo_my_model.covar_module_C.base_kernel.raw_lengthscale.requires_grad = False\n",
    "\n",
    "    # optimizer and scheduler\n",
    "    if train_style == 'all-together':\n",
    "        model_params_to_optimize = [\n",
    "            param for name, param in wo_my_model.named_parameters()\n",
    "            if not name.startswith('covar_module_X') and not name.startswith('covar_module_C')\n",
    "        ]\n",
    "\n",
    "        wo_optimizer = torch.optim.Adam([\n",
    "        {'params': model_params_to_optimize},\n",
    "        {'params': wo_likelihood.parameters()} # likelihood parameter is fixed or not.\n",
    "        ], lr=0.3)\n",
    "\n",
    "        wo_scheduler = StepLR(wo_optimizer, step_size=50, gamma=0.95)  # 0.3, 50, 0.95; \n",
    "\n",
    "    elif train_style == 'em-style':\n",
    "\n",
    "        variational_params = [\n",
    "            param for name, param in wo_my_model.named_parameters()\n",
    "            if name.startswith('variational_strategy')\n",
    "        ]\n",
    "\n",
    "        x_params = [\n",
    "            param for name, param in wo_my_model.named_parameters()\n",
    "            if name.startswith('X')\n",
    "        ]\n",
    "\n",
    "        wo_optimizer_e_step = torch.optim.Adam(variational_params, lr=0.3)\n",
    "        wo_scheduler_e_step = StepLR(wo_optimizer_e_step, step_size=50, gamma=0.95)\n",
    "\n",
    "        wo_optimizer_m_step = torch.optim.Adam([{'params': x_params}, {'params': wo_likelihood.parameters()}], lr=0.3)\n",
    "        wo_scheduler_m_step = StepLR(wo_optimizer_m_step, step_size=50, gamma=0.95)\n",
    "\n",
    "elif train_parameters == 'all-parameters':\n",
    "    \n",
    "    # optimizer and scheduler\n",
    "    if train_style == 'all-together':\n",
    "        \n",
    "        wo_optimizer = torch.optim.Adam([\n",
    "        {'params': wo_my_model.parameters()},\n",
    "        {'params': wo_likelihood.parameters()} # likelihood parameter is fixed or not.\n",
    "        ], lr=0.3)\n",
    "\n",
    "        wo_scheduler = StepLR(wo_optimizer, step_size=50, gamma=0.95)  # 0.3, 50, 0.95; \n",
    "        \n",
    "    elif train_style == 'em-style':\n",
    "\n",
    "        variational_params = [\n",
    "            param for name, param in wo_my_model.named_parameters()\n",
    "            if name.startswith('variational_strategy')\n",
    "        ]\n",
    "\n",
    "        covar_and_x_params = [\n",
    "            param for name, param in wo_my_model.named_parameters()\n",
    "            if name.startswith('covar_module') or name.startswith('X')\n",
    "        ]\n",
    "\n",
    "        wo_optimizer_e_step = torch.optim.Adam(variational_params, lr=0.3)\n",
    "        wo_scheduler_e_step = StepLR(wo_optimizer_e_step, step_size=50, gamma=0.95)\n",
    "        \n",
    "        wo_optimizer_m_step = torch.optim.Adam([{'params': covar_and_x_params}, {'params': wo_likelihood.parameters()}], lr=0.3)\n",
    "        wo_scheduler_m_step = StepLR(wo_optimizer_m_step, step_size=50, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_style == 'all-together':\n",
    "    # start training!\n",
    "    wo_loss_list = []\n",
    "    n_iterations = 1500 # 10000\n",
    "    iterator = trange(n_iterations, leave=True)\n",
    "    batch_size_X = 20\n",
    "    batch_size_C = wo_n_C_train # how many training inputs\n",
    "    wo_model_max_grad_norm = 15\n",
    "    wo_likeli_max_grad_norm = 0.7\n",
    "\n",
    "    wo_my_model.train()\n",
    "    wo_likelihood.train()\n",
    "    for i in iterator: \n",
    "        # The following step ensures samples C are all 'valid' training ones.\n",
    "        batch_index_X, batch_index_C = sample_index_X_and_C_from_list(wo_ls_of_ls_train_C, batch_size_X, batch_size_C)\n",
    "        # core code is here \n",
    "        wo_optimizer.zero_grad()\n",
    "        # sample_batch_X = wo_my_model.sample_latent_variable(batch_idx=None)\n",
    "        sample_X = wo_my_model.sample_latent_variable()  # a full sample returns latent x across all n_X, not scalable when n_X is large TODO: more efficient?\n",
    "        sample_batch_X = sample_X[batch_index_X]\n",
    "        sample_batch_C = wo_C_total[batch_index_C]\n",
    "        output_batch = wo_my_model(sample_batch_X, sample_batch_C.reshape(-1,1)) # q(f)\n",
    "        batch_index_Y = inhomogeneous_index_of_batch_Y(batch_index_X, batch_index_C, wo_n_X_model, wo_n_C_total) # n_C_total is because all c index considered are based on C_total.\n",
    "        loss = -wo_mll(output_batch, wo_sample_total_data[batch_index_Y]).sum()\n",
    "        wo_loss_list.append(loss.item())\n",
    "        iterator.set_description('Loss: ' + str(float(np.round(loss.item(),3))) + \", iter no: \" + str(i))\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(wo_my_model.parameters(), wo_model_max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(wo_likelihood.parameters(), wo_likeli_max_grad_norm)\n",
    "\n",
    "        wo_optimizer.step()\n",
    "        wo_scheduler.step()\n",
    "\n",
    "elif train_style == 'em-style':\n",
    "    \n",
    "    # start training!\n",
    "    wo_loss_list = []\n",
    "    n_iterations = 3000 # 10000\n",
    "    iterator = trange(n_iterations, leave=True)\n",
    "    batch_size_X = 20\n",
    "    batch_size_C = 50\n",
    "    wo_model_max_grad_norm = 15\n",
    "    wo_likeli_max_grad_norm = 0.7\n",
    "\n",
    "    wo_my_model.train()\n",
    "    wo_likelihood.train()\n",
    "    iteration_tracker = 0\n",
    "    for i in iterator: \n",
    "        # The following step ensures samples C are all 'valid' training ones.\n",
    "        batch_index_X, batch_index_C = sample_index_X_and_C_from_list(wo_ls_of_ls_train_C, batch_size_X, batch_size_C)\n",
    "        # core code is here \n",
    "        wo_optimizer_e_step.zero_grad()\n",
    "        wo_optimizer_m_step.zero_grad()\n",
    "        sample_X = wo_my_model.sample_latent_variable()  # a full sample returns latent x across all n_X TODO: more efficient?\n",
    "        sample_batch_X = sample_X[batch_index_X]\n",
    "        sample_batch_C = wo_C_total[batch_index_C]\n",
    "        output_batch = wo_my_model(sample_batch_X, sample_batch_C) # q(f)\n",
    "        batch_index_Y = inhomogeneous_index_of_batch_Y(batch_index_X, batch_index_C, wo_n_X_model, wo_n_C_total) # n_C_total is because all c index considered are based on C_total.\n",
    "        loss = -wo_mll(output_batch, wo_sample_total_data[batch_index_Y]).sum()\n",
    "        wo_loss_list.append(loss.item())\n",
    "        iterator.set_description('Loss: ' + str(float(np.round(loss.item(),3))) + \", iter no: \" + str(i))\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(wo_my_model.parameters(), wo_model_max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(wo_likelihood.parameters(), wo_likeli_max_grad_norm)\n",
    "\n",
    "        \n",
    "        if iteration_tracker <= 60: \n",
    "            wo_optimizer_e_step.step()\n",
    "            wo_scheduler_e_step.step()\n",
    "            iteration_tracker += 1\n",
    "\n",
    "        elif iteration_tracker > 60 and iteration_tracker <= 100:\n",
    "            wo_optimizer_m_step.step()\n",
    "            wo_scheduler_m_step.step()\n",
    "\n",
    "            if iteration_tracker != 100:\n",
    "                iteration_tracker += 1\n",
    "            else:\n",
    "                iteration_tracker = 0\n",
    "        '''\n",
    "        if i % 3 == 1 or i % 3 == 2:\n",
    "            wo_optimizer_e_step.step()\n",
    "            wo_scheduler_e_step.step()\n",
    "        else:\n",
    "            wo_optimizer_m_step.step()\n",
    "            wo_scheduler_m_step.step()\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(wo_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After Training, have a look at fitted kernel parameters...\\n')\n",
    "\n",
    "print('model covar_module_X raw output_scale\\n', wo_my_model.covar_module_X.raw_outputscale.data)\n",
    "print('model covar_module_X base kernel raw lengthscale\\n', wo_my_model.covar_module_X.base_kernel.raw_lengthscale.data)\n",
    "print('model covar_module_C raw outputscale\\n', wo_my_model.covar_module_C.raw_outputscale.data)\n",
    "print('model covar_module_C base_kernel raw lengthscale\\n', wo_my_model.covar_module_C.base_kernel.raw_lengthscale.data)\n",
    "print('likelihood noise_covar raw noise', wo_likelihood.noise_covar.raw_noise.data)\n",
    "\n",
    "print('----- ----- ' * 10)\n",
    "\n",
    "print('model covar_module_X output_scale\\n', wo_my_model.covar_module_X.outputscale.data)\n",
    "print('model covar_module_X base kernel lengthscale\\n', wo_my_model.covar_module_X.base_kernel.lengthscale.data)\n",
    "print('model covar_module_C outputscale\\n', wo_my_model.covar_module_C.outputscale.data)\n",
    "print('model covar_module_C base_kernel lengthscale\\n', wo_my_model.covar_module_C.base_kernel.lengthscale.data)\n",
    "print('likelihood noise_covar noise', wo_likelihood.noise_covar.noise.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction output for all grid (total) inputs.\n",
    "wo_my_model.eval()\n",
    "wo_likelihood.eval()\n",
    "\n",
    "batch_size_X = wo_n_outputs\n",
    "batch_size_C = wo_n_C_total # this number must equal to n_total = 700 !\n",
    "sample_X = wo_my_model.X.q_mu.data # TODO: try other meaningful approaches\n",
    "\n",
    "# indices for all inputs.\n",
    "batch_index_X = np.array([[i]*batch_size_C for i in range(batch_size_X)]).reshape(-1).tolist() \n",
    "batch_index_C = [i for i in range(batch_size_C)] * batch_size_X \n",
    "\n",
    "assert len(batch_index_X) == len(batch_index_C)\n",
    "\n",
    "sample_batch_X = sample_X[batch_index_X]\n",
    "sample_batch_C = wo_C_total[batch_index_C]\n",
    "# NOTE: predictions for ALL inputs. \n",
    "wo_grid_output_batch = wo_my_model(sample_batch_X, sample_batch_C) # q(f)\n",
    "# passing through likelihood.\n",
    "wo_grid_output_batch = wo_likelihood(wo_grid_output_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# prediction output for all grid (total) inputs.\n",
    "wo_my_model.eval()\n",
    "wo_likelihood.eval()\n",
    "\n",
    "batch_size_X = wo_n_outputs\n",
    "batch_size_C = wo_n_C_total # this number must equal to n_total = 700!\n",
    "mean_tensor = wo_my_model.X.q_mu.data \n",
    "log_sigma_tensor = wo_my_model.X.q_log_sigma.data\n",
    "sample_X_tensor = sample_from_multivariantgaussian(mean_tensor, log_sigma_tensor, monte_carlo_samples=3)\n",
    "\n",
    "# indices for all inputs.\n",
    "batch_index_X = np.array([[i]*batch_size_C for i in range(batch_size_X)]).reshape(-1).tolist() \n",
    "batch_index_C = [i for i in range(batch_size_C)] * batch_size_X \n",
    "\n",
    "assert len(batch_index_X) == len(batch_index_C)\n",
    "\n",
    "_, wo_grid_output_batch = mc_pred_helper(wo_my_model, wo_likelihood, sample_X_tensor, wo_C_total, batch_index_X, batch_index_C)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test RMSE (Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_train_data_predict = wo_grid_output_batch.loc.detach()[wo_sample_train_index]\n",
    "train_rmse = (wo_train_data_predict - wo_sample_train_data).square().mean().sqrt()\n",
    "print('Global Train RMSE', train_rmse)\n",
    "\n",
    "wo_test_data_predict = wo_grid_output_batch.loc.detach()[wo_sample_test_index]\n",
    "test_rmse = (wo_test_data_predict - wo_sample_test_data).square().mean().sqrt()\n",
    "print('Global Test RMSE', test_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the index of the funtion to show\n",
    "wo_function_index = 17 # 0 - 19 (in total 20 output functions)\n",
    "\n",
    "# Train\n",
    "wo_train_input = wo_C_total[wo_train_C_tidily_indices]\n",
    "wo_train_start = wo_train_input.shape[0] * wo_function_index\n",
    "wo_train_end = wo_train_start + wo_train_input.shape[0]\n",
    "wo_train_target = wo_sample_train_data[wo_train_start:wo_train_end]\n",
    "\n",
    "wo_train_pred_mean = wo_train_data_predict[wo_train_start:wo_train_end]\n",
    "wo_train_pred_std = wo_grid_output_batch.stddev.detach()[wo_sample_train_index][wo_train_start:wo_train_end]\n",
    "print('Train RMSE', (wo_train_pred_mean - wo_train_target).square().mean().sqrt())\n",
    "\n",
    "\n",
    "# Test\n",
    "wo_test_input = wo_C_total[wo_test_C_tidily_indices]\n",
    "wo_test_start = wo_test_input.shape[0] * wo_function_index\n",
    "wo_test_end = wo_test_start + wo_test_input.shape[0]\n",
    "wo_test_target = wo_sample_test_data[wo_test_start:wo_test_end]\n",
    "\n",
    "wo_test_pred_mean = wo_test_data_predict[wo_test_start:wo_test_end]\n",
    "wo_test_pred_std = wo_grid_output_batch.stddev.detach()[wo_sample_test_index][wo_test_start:wo_test_end]\n",
    "print('Test RMSE', (wo_test_pred_mean - wo_test_target).square().mean().sqrt())\n",
    "\n",
    "# Total\n",
    "wo_gp_input = wo_C_total\n",
    "wo_gp_start = wo_gp_input.shape[0] * wo_function_index\n",
    "wo_gp_end = wo_gp_start + wo_gp_input.shape[0]\n",
    "wo_gp_target = wo_sample_total_data[wo_gp_start:wo_gp_end]\n",
    "\n",
    "wo_gp_pred_mean = wo_grid_output_batch.loc.detach()[wo_gp_start:wo_gp_end]\n",
    "wo_gp_pred_std = wo_grid_output_batch.stddev.detach()[wo_gp_start:wo_gp_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traindata_testdata_fittedgp(train_X=wo_train_input, train_Y=wo_train_target, test_X=wo_test_input, test_Y=wo_test_target, gp_X=wo_gp_input, gp_pred_mean=wo_gp_pred_mean, gp_pred_std=wo_gp_pred_std, inducing_points_X=wo_my_model.variational_strategy.inducing_points_C.detach(), n_inducing_C=wo_n_inducing_C) # NOTE: input is C not X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True v.s. Fitted latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_and_fitted_latent(wo_X_true, wo_my_model.X.q_mu.detach(), torch.nn.functional.softplus(wo_my_model.X.q_log_sigma.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expri_random_seed = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is same as 'no missing data' case.\n",
    "w_n_C_total = 700 # totally 700 points for C\n",
    "w_n_outputs = 20\n",
    "w_X_true, w_C_total, w_sample_total_data, kernel_parameters = tidily_sythetic_data_from_MOGP(n_C=w_n_C_total, n_X=w_n_outputs)\n",
    "\n",
    "w_n_C_train = 20 # the number of training data points per output\n",
    "w_n_C_test = w_n_C_total - w_n_C_train\n",
    "\n",
    "np.random.seed(expri_random_seed)\n",
    "list_expri_random_seeds = np.random.randn(w_n_outputs)\n",
    "print(list_expri_random_seeds)\n",
    "\n",
    "# different from the previous case, C_train and C_test no longer a single set, but every output has different values.\n",
    "w_ls_of_ls_train_C = []\n",
    "w_ls_of_ls_test_C = []\n",
    "\n",
    "w_sample_train_index, w_sample_test_index = [], []\n",
    "\n",
    "for i in range(w_n_outputs):\n",
    "    # iterate across different output functions\n",
    "    random.seed(list_expri_random_seeds[i])\n",
    "    train_index = random.sample(range(w_n_C_total), w_n_C_train)\n",
    "    test_index = [index for index in range(w_n_C_total) if index not in train_index]\n",
    "    w_ls_of_ls_train_C.append(train_index)\n",
    "    w_ls_of_ls_test_C.append(test_index)\n",
    "    '''\n",
    "    C_train = C_total[train_index]\n",
    "    C_test = C_total[test_index]\n",
    "    assert C_train.shape[0] == n_train\n",
    "    assert C_test.shape[0] == n_test\n",
    "    C_train_List.append(C_train)\n",
    "    C_test_List.append(C_test)\n",
    "    '''\n",
    "    w_sample_train_index = np.concatenate((w_sample_train_index, list(np.array(train_index) + w_n_C_total*i)))\n",
    "    w_sample_test_index = np.concatenate((w_sample_test_index, list(np.array(test_index) + w_n_C_total*i)))\n",
    "\n",
    "w_sample_train_data = w_sample_total_data[w_sample_train_index]\n",
    "w_sample_test_data = w_sample_total_data[w_sample_test_index]\n",
    "\n",
    "assert w_sample_train_data.shape[0] == w_n_C_train * w_n_outputs\n",
    "assert w_sample_test_data.shape[0] == w_n_C_test * w_n_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_functions import sample_index_X_and_C_from_list\n",
    "# define hyper-parameters\n",
    "w_n_X = w_X_true.shape[0]\n",
    "w_n_C = len(w_ls_of_ls_train_C[0])\n",
    "w_n_total = w_n_X * w_n_C\n",
    "w_index_dim = 1\n",
    "w_latent_dim = 2\n",
    "w_n_inducing_C = 15\n",
    "w_n_inducing_X = 30\n",
    "w_pca = False\n",
    "\n",
    "Y_train = w_sample_train_data\n",
    "\n",
    "# specify model\n",
    "w_my_model = LVMOGP_SVI(w_n_X, w_n_C, w_index_dim, w_latent_dim, w_n_inducing_C, w_n_inducing_X, Y_train.reshape(w_n_X, -1), pca=w_pca)\n",
    "\n",
    "# Likelihood & training objective\n",
    "w_likelihood = GaussianLikelihood()\n",
    "w_mll = VariationalELBO(w_likelihood, w_my_model, num_data=w_n_total)\n",
    "\n",
    "# load in partially-trained model\n",
    "load_model = False\n",
    "load_likelihood = False\n",
    "if load_model:\n",
    "    model_path = '/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/models/model_weights.pth'\n",
    "    state_dict = torch.load(model_path)\n",
    "    w_my_model.load_state_dict(state_dict)\n",
    "\n",
    "if load_likelihood:\n",
    "    likelihood_path = '/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/models/likelihood_weights.pth'\n",
    "    state_dict = torch.load(likelihood_path)\n",
    "    w_likelihood.load_state_dict(state_dict)\n",
    "\n",
    "# optimizer and scheduler\n",
    "w_optimizer = torch.optim.Adam([\n",
    "    {'params': w_my_model.parameters()},\n",
    "    {'params': w_likelihood.parameters()} \n",
    "], lr=0.3)\n",
    "\n",
    "w_scheduler = StepLR(w_optimizer, step_size=50, gamma=0.95)  # every 50 iterations，learning rate multiple 0.95\n",
    "\n",
    "# have a look at parameters\n",
    "for name, param in w_my_model.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "print('---' * 15) \n",
    "for name, param in w_likelihood.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# Initialize inducing points in C space\n",
    "w_my_model.variational_strategy.inducing_points_C.data = torch.rand(w_n_inducing_C).reshape(-1,1) * 20 - 10\n",
    "# This depends on interval (-10,10) appear in tidily_sythetic_data_from_MOGP\n",
    "\n",
    "# print('have a look at BEFORE training model parameters ... ')\n",
    "# print('inducing_points_X', w_my_model.variational_strategy.inducing_points_X.detach())\n",
    "# print('inducing_points_C', w_my_model.variational_strategy.inducing_points_C.detach())\n",
    "# print('X q_mu', w_my_model.X.q_mu.detach())\n",
    "# print('X q_sigma', w_my_model.X.q_log_sigma.detach().exp())\n",
    "# print('variational mean', w_my_model.variational_strategy._variational_distribution.variational_mean.detach())\n",
    "# print('variational chol X', w_my_model.variational_strategy._variational_distribution.chol_variational_covar_X.detach())\n",
    "# print('variational chol C', w_my_model.variational_strategy._variational_distribution.chol_variational_covar_C.detach())\n",
    "# print('covar_module_X outputscale', w_my_model.covar_module_X.outputscale.detach())\n",
    "# print('covar_module_X base_kernel lengthscale', w_my_model.covar_module_X.base_kernel.lengthscale.detach())\n",
    "# print('covar_module_C outputscale', w_my_model.covar_module_C.outputscale.detach())\n",
    "# print('covar_module_C base_kernel lengthscale', w_my_model.covar_module_C.base_kernel.lengthscale.detach())\n",
    "\n",
    "# start training!\n",
    "w_loss_list = []\n",
    "n_iterations = 1500 # 10000\n",
    "iterator = trange(n_iterations, leave=True)\n",
    "batch_size_X = 20\n",
    "batch_size_C = w_n_C_train # 50\n",
    "w_model_max_grad_norm = 15\n",
    "w_likeli_max_grad_norm = 0.7\n",
    "\n",
    "w_my_model.train()\n",
    "w_likelihood.train()\n",
    "for i in iterator: \n",
    "    batch_index_X, batch_index_C = sample_index_X_and_C_from_list(w_ls_of_ls_train_C, batch_size_X=batch_size_X, batch_size_C=batch_size_C)\n",
    "    # core code is here \n",
    "    w_optimizer.zero_grad()\n",
    "    sample_X = w_my_model.sample_latent_variable()  # a full sample returns latent x across all n_X TODO: more efficient?\n",
    "    sample_batch_X = sample_X[batch_index_X]\n",
    "    sample_batch_C = w_C_total[batch_index_C]\n",
    "    output_batch = w_my_model(sample_batch_X, sample_batch_C) # q(f)\n",
    "    batch_index_Y = inhomogeneous_index_of_batch_Y(batch_index_X, batch_index_C, w_n_X, w_n_C_total)\n",
    "    loss = -w_mll(output_batch, w_sample_total_data[batch_index_Y]).sum()\n",
    "    w_loss_list.append(loss.item())\n",
    "    iterator.set_description('Loss: ' + str(float(np.round(loss.item(),3))) + \", iter no: \" + str(i))\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(w_my_model.parameters(), w_model_max_grad_norm)\n",
    "    torch.nn.utils.clip_grad_norm_(w_likelihood.parameters(), w_likeli_max_grad_norm)\n",
    "\n",
    "    w_optimizer.step()\n",
    "    w_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('have a look at AFTER training model parameters ... ')\n",
    "\n",
    "# print('inducing_points_X', w_my_model.variational_strategy.inducing_points_X.detach())\n",
    "# print('inducing_points_C', w_my_model.variational_strategy.inducing_points_C.detach())\n",
    "# print('X q_mu', w_my_model.X.q_mu.detach())\n",
    "# print('X q_sigma', w_my_model.X.q_log_sigma.detach().exp())\n",
    "# print('variational mean', w_my_model.variational_strategy._variational_distribution.variational_mean.detach())\n",
    "# print('variational chol X', w_my_model.variational_strategy._variational_distribution.chol_variational_covar_X.detach())\n",
    "# print('variational chol C', w_my_model.variational_strategy._variational_distribution.chol_variational_covar_C.detach())\n",
    "# print('covar_module_X outputscale', w_my_model.covar_module_X.outputscale.detach())\n",
    "# print('covar_module_X base_kernel lengthscale', w_my_model.covar_module_X.base_kernel.lengthscale.detach())\n",
    "# print('covar_module_C outputscale', w_my_model.covar_module_C.outputscale.detach())\n",
    "# print('covar_module_C base_kernel lengthscale', w_my_model.covar_module_C.base_kernel.lengthscale.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(w_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction output for grid (total) inputs.\n",
    "w_my_model.eval()\n",
    "w_likelihood.eval()\n",
    "\n",
    "batch_size_X = 20\n",
    "batch_size_C = 700 # this number must equal to n_total = 700 !\n",
    "sample_X = w_my_model.X.q_mu # TODO: try other meaningful approaches, such as monte carlo samling approximation ...\n",
    "\n",
    "batch_index_X = np.array([[i]*batch_size_C for i in range(batch_size_X)]).reshape(-1).tolist() \n",
    "batch_index_C = [i for i in range(batch_size_C)] * batch_size_X \n",
    "\n",
    "assert len(batch_index_X) == len(batch_index_C)\n",
    "\n",
    "sample_batch_X = sample_X[batch_index_X]\n",
    "sample_batch_C = w_C_total[batch_index_C]\n",
    "w_grid_output_batch = w_my_model(sample_batch_X, sample_batch_C) # q(f)\n",
    "# passing through likelihood\n",
    "w_grid_output_batch = w_likelihood(w_grid_output_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test data RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train_data_predict = w_grid_output_batch.loc.detach()[w_sample_train_index]\n",
    "train_rmse = (w_train_data_predict - w_sample_train_data).square().mean().sqrt()\n",
    "print('Train RMSE', train_rmse)\n",
    "\n",
    "w_test_data_predict = w_grid_output_batch.loc.detach()[w_sample_test_index]\n",
    "test_rmse = (w_test_data_predict - w_sample_test_data).square().mean().sqrt()\n",
    "print('Test RMSE', test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the index of the funtion to show\n",
    "w_function_index = 13 # 0 - 19 (in total 20 output functions)\n",
    "\n",
    "w_train_input = w_C_total[w_ls_of_ls_train_C[w_function_index]]\n",
    "w_train_start = 0\n",
    "for i in range(w_function_index):\n",
    "    w_train_start += len(w_ls_of_ls_train_C[i]) # don't assume every output has the same length of inputs\n",
    "w_train_end = w_train_start + len(w_ls_of_ls_train_C[w_function_index])\n",
    "w_train_target = w_sample_train_data[w_train_start:w_train_end]\n",
    "w_train_predict = w_train_data_predict[w_train_start:w_train_end]\n",
    "train_rmse = (w_train_target - w_train_predict).square().mean().sqrt()\n",
    "print('train rmse', train_rmse)\n",
    "\n",
    "w_test_input = w_C_total[w_ls_of_ls_test_C[w_function_index]]\n",
    "w_test_start = 0\n",
    "for j in range(w_function_index):\n",
    "    w_test_start += len(w_ls_of_ls_test_C[i])\n",
    "w_test_end = w_test_start + len(w_ls_of_ls_test_C[w_function_index])\n",
    "w_test_target = w_sample_test_data[w_test_start:w_test_end]\n",
    "w_test_predict = w_test_data_predict[w_test_start:w_test_end]\n",
    "test_rmse = (w_test_predict - w_test_target).square().mean().sqrt()\n",
    "print('test rmse', test_rmse)\n",
    "\n",
    "w_gp_input = w_C_total\n",
    "w_gp_start = w_gp_input.shape[0] * w_function_index\n",
    "w_gp_end = w_gp_start + w_gp_input.shape[0]\n",
    "w_gp_target = w_sample_total_data[w_gp_start:w_gp_end]\n",
    "\n",
    "\n",
    "w_gp_pred_mean = w_grid_output_batch.loc.detach()[w_gp_start:w_gp_end]\n",
    "w_gp_pred_std = w_grid_output_batch.stddev.detach()[w_gp_start:w_gp_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traindata_testdata_fittedgp(train_X=w_train_input, train_Y=w_train_target, test_X=w_test_input, test_Y=w_test_target, gp_X=w_gp_input, gp_pred_mean=w_gp_pred_mean, gp_pred_std=w_gp_pred_std, inducing_points_X=w_my_model.variational_strategy.inducing_points_C.detach()) # NOTE: input is C not X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True v.s. Fitted latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_and_fitted_latent(w_X_true, w_my_model.X.q_mu.detach(), torch.nn.functional.softplus(w_my_model.X.q_log_sigma.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
