{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from linear_operator.operators import KroneckerProductLinearOperator\n",
    "from torch import Tensor\n",
    "from torch.distributions import MultivariateNormal\n",
    "import sys\n",
    "sys.path.append('/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/')\n",
    "from models_.lvmogp_svi import LVMOGP_SVI\n",
    "from models_.gaussian_likelihood import GaussianLikelihood\n",
    "from models_.variational_elbo import VariationalELBO\n",
    "from tqdm import trange\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from util_functions import *\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is same as 'no missing data' case.\n",
    "idgp_w_n_C_total = 50 # totally 700 points for C\n",
    "idgp_w_n_outputs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expri_random_seed = 13\n",
    "\n",
    "np.random.seed(expri_random_seed)\n",
    "list_expri_random_seeds = np.random.randn(idgp_w_n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data_path = f'/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/data/synth_regression/ninputs_{idgp_w_n_C_total}_nlatents_{idgp_w_n_outputs}'\n",
    "idgp_w_C_total = Tensor(pd.read_csv(f'{synth_data_path}/inputs.csv').to_numpy()).reshape(-1)\n",
    "idgp_w_X_true = Tensor(pd.read_csv(f'{synth_data_path}/latents.csv').to_numpy()).reshape(-1, 2)\n",
    "idgp_w_sample_total_data = Tensor(pd.read_csv(f'{synth_data_path}/target_data.csv').to_numpy()).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code generates all output data.\n",
    "# idgp_w_X_true, idgp_w_C_total, idgp_w_sample_total_data, kernel_parameters = tidily_sythetic_data_from_MOGP(n_C=idgp_w_n_C_total, n_X=idgp_w_n_outputs)\n",
    "\n",
    "idgp_w_n_C_train = 25 # the number of training data points per output\n",
    "idgp_w_n_C_test = idgp_w_n_C_total - idgp_w_n_C_train\n",
    "\n",
    "# different from the previous case, C_train and C_test no longer a single set, but every output has different values.\n",
    "idgp_w_ls_of_ls_train_C = []\n",
    "idgp_w_ls_of_ls_test_C = []\n",
    "\n",
    "idgp_w_sample_train_index, idgp_w_sample_test_index = [], []\n",
    "\n",
    "for i in range(idgp_w_n_outputs):\n",
    "    # iterate across different output functions\n",
    "    random.seed(list_expri_random_seeds[i])\n",
    "    train_index = random.sample(range(idgp_w_n_C_total), idgp_w_n_C_train)\n",
    "    test_index = [index for index in range(idgp_w_n_C_total) if index not in train_index]\n",
    "    idgp_w_ls_of_ls_train_C.append(train_index)\n",
    "    idgp_w_ls_of_ls_test_C.append(test_index)\n",
    "    \n",
    "    idgp_w_sample_train_index = np.concatenate((idgp_w_sample_train_index, list(np.array(train_index) + idgp_w_n_C_total*i)))\n",
    "    idgp_w_sample_test_index = np.concatenate((idgp_w_sample_test_index, list(np.array(test_index) + idgp_w_n_C_total*i)))\n",
    "\n",
    "idgp_w_sample_train_data = idgp_w_sample_total_data[idgp_w_sample_train_index]\n",
    "idgp_w_sample_test_data = idgp_w_sample_total_data[idgp_w_sample_test_index]\n",
    "\n",
    "# data for all outouts ...\n",
    "assert idgp_w_sample_train_data.shape[0] == idgp_w_n_C_train * idgp_w_n_outputs\n",
    "assert idgp_w_sample_test_data.shape[0] == idgp_w_n_C_test * idgp_w_n_outputs\n",
    "assert idgp_w_sample_total_data.shape[0] == idgp_w_n_C_total * idgp_w_n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_idgp_function_index = [i for i in range(idgp_w_n_outputs)]\n",
    "list_train_X, list_train_Y = [], [] \n",
    "list_test_X, list_test_Y = [], []\n",
    "list_total_X, list_total_Y = [], [] \n",
    "\n",
    "for idgp_function_index in list_idgp_function_index:\n",
    "    # start and end\n",
    "    idgp_train_start = idgp_function_index * idgp_w_n_C_train\n",
    "    idgp_train_end = idgp_train_start + idgp_w_n_C_train\n",
    "\n",
    "    idgp_test_start = idgp_function_index * idgp_w_n_C_test\n",
    "    idgp_test_end = idgp_test_start + idgp_w_n_C_test\n",
    "\n",
    "    idgp_total_start = idgp_function_index * idgp_w_n_C_total\n",
    "    idgp_total_end = idgp_total_start + idgp_w_n_C_total\n",
    "\n",
    "    # training data\n",
    "    train_X = idgp_w_C_total[idgp_w_ls_of_ls_train_C[idgp_function_index]]\n",
    "    train_Y = idgp_w_sample_train_data[idgp_train_start:idgp_train_end]\n",
    "    assert train_X.shape ==  train_Y.shape == torch.Size([idgp_w_n_C_train])\n",
    "    list_train_X.append(train_X)\n",
    "    list_train_Y.append(train_Y)\n",
    "\n",
    "    # testing data\n",
    "    test_X = idgp_w_C_total[idgp_w_ls_of_ls_test_C[idgp_function_index]]\n",
    "    test_Y = idgp_w_sample_test_data[idgp_test_start:idgp_test_end]\n",
    "    assert test_X.shape == test_Y.shape == torch.Size([idgp_w_n_C_test]) \n",
    "    list_test_X.append(test_X)\n",
    "    list_test_Y.append(test_Y)\n",
    "\n",
    "    # total data, of length idgp_w_n_C_total\n",
    "    total_X = idgp_w_C_total\n",
    "    total_Y = idgp_w_sample_total_data[idgp_total_start:idgp_total_end]\n",
    "    assert total_X.shape == total_Y.shape == torch.Size([idgp_w_n_C_total])\n",
    "    list_total_X.append(total_X)\n",
    "    list_total_Y.append(total_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "# Model\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "idgp_n_inducing_C = 20\n",
    "learning_rate = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train all gps one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_.gaussian_likelihood import GaussianLikelihood\n",
    "from models_.variational_elbo import VariationalELBO\n",
    "\n",
    "list_models = []\n",
    "list_likelihoods = []\n",
    "list_idgp_loss_list = []\n",
    "\n",
    "for j in range(idgp_w_n_outputs):\n",
    "\n",
    "    # current train_X and train_Y\n",
    "    train_X = list_train_X[j]\n",
    "    train_Y = list_train_Y[j]\n",
    "    \n",
    "    inducing_points = torch.rand(idgp_n_inducing_C).reshape(-1,1) * 20 - 10\n",
    "    model = GPModel(inducing_points=inducing_points)\n",
    "    likelihood = GaussianLikelihood()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=learning_rate)\n",
    "\n",
    "    wo_scheduler = StepLR(optimizer, step_size=50, gamma=0.95)  # 0.3, 50, 0.95; \n",
    "\n",
    "    # Our loss object. We're using the VariationalELBO\n",
    "    mll = VariationalELBO(likelihood, model, num_data=train_Y.size(0))\n",
    "\n",
    "    # have a look at parameters\n",
    "    # for name, param in model.named_parameters():\n",
    "    #    print(name, param.size())\n",
    "        \n",
    "    # start training!\n",
    "    idgp_loss_list = []\n",
    "    n_iterations = 5000 # 1500 # 10000\n",
    "    iterator = trange(n_iterations, leave=True)\n",
    "    idgp_model_max_grad_norm = 15\n",
    "    idgp_likeli_max_grad_norm = 0.7\n",
    "\n",
    "    for i in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output_pred = model(train_X)\n",
    "        loss = -mll(output_pred, train_Y)\n",
    "        idgp_loss_list.append(loss.item())\n",
    "        iterator.set_description( 'Training '+ str(j) + 'th Model; '+ 'Loss: ' + str(float(np.round(loss.item(),3))) + \", iter no: \" + str(i))\n",
    "        loss.backward()\n",
    "\n",
    "        # clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), idgp_model_max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(likelihood.parameters(), idgp_likeli_max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        wo_scheduler.step()\n",
    "\n",
    "    # After traininhg, store all the models ... \n",
    "    list_models.append(model)\n",
    "    list_likelihoods.append(likelihood)\n",
    "    list_idgp_loss_list.append(idgp_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all gp testing results one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_idgp_train_output_dist = []\n",
    "list_idgp_test_output_dist = []\n",
    "list_idgp_total_output_dist = []\n",
    "\n",
    "train_error_square_sum = 0\n",
    "test_error_square_sum = 0\n",
    "\n",
    "train_error_length = 0\n",
    "test_error_length = 0\n",
    "\n",
    "for j in range(idgp_w_n_outputs):\n",
    "    curr_model = list_models[j]\n",
    "    curr_likelihood = list_likelihoods[j]\n",
    "\n",
    "    curr_train_X = list_train_X[j]\n",
    "    curr_train_Y = list_train_Y[j]\n",
    "\n",
    "    curr_test_X = list_test_X[j]\n",
    "    curr_test_Y = list_test_Y[j]\n",
    "\n",
    "    curr_total_X = list_total_X[j]\n",
    "    curr_total_Y = list_total_Y[j]\n",
    "\n",
    "    idgp_train_output_dist = curr_likelihood(curr_model(curr_train_X))\n",
    "    idgp_test_output_dist  = curr_likelihood(curr_model(curr_test_X))\n",
    "    idgp_total_output_dist = curr_likelihood(curr_model(curr_total_X))\n",
    "\n",
    "    # RMSE\n",
    "    train_error_square_sum += (idgp_train_output_dist.loc.detach() - curr_train_Y).square().sum()\n",
    "    print(str(j) + 'th Model Train RMSE: ', (idgp_train_output_dist.loc.detach() - curr_train_Y).square().mean().sqrt())\n",
    "    test_error_square_sum += (idgp_test_output_dist.loc.detach() - curr_test_Y).square().sum()\n",
    "    print(str(j) + 'th Model Test RMSE: ', (idgp_test_output_dist.loc.detach() - curr_test_Y).square().mean().sqrt())\n",
    "\n",
    "    # NLL\n",
    "    train_nll_ = neg_log_likelihood(curr_train_Y, idgp_train_output_dist.loc.detach(), idgp_train_output_dist.variance.detach())\n",
    "    test_nll_ = neg_log_likelihood(curr_test_Y, idgp_test_output_dist.loc.detach(), idgp_test_output_dist.variance.detach())\n",
    "    print(str(j) + 'th Model Train NLL: ', train_nll_)\n",
    "    print(str(j) + 'th Model Test NLL: ', test_nll_)\n",
    "\n",
    "    train_error_length += idgp_train_output_dist.loc.detach().shape[0]\n",
    "    test_error_length += idgp_test_output_dist.loc.detach().shape[0]    \n",
    "\n",
    "    list_idgp_train_output_dist.append(idgp_train_output_dist)\n",
    "    list_idgp_test_output_dist.append(idgp_test_output_dist)\n",
    "    list_idgp_total_output_dist.append(idgp_total_output_dist)\n",
    "\n",
    "\n",
    "print('Global Train RMSE', (train_error_square_sum / train_error_length).sqrt())\n",
    "print('Global Test RMSE', (test_error_square_sum / test_error_length).sqrt())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select one output, and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_output_index = 13\n",
    "\n",
    "selected_model = list_models[selected_output_index]\n",
    "selected_train_X = list_train_X[selected_output_index]\n",
    "selected_train_Y = list_train_Y[selected_output_index]\n",
    "selected_test_X = list_test_X[selected_output_index]\n",
    "selected_test_Y = list_test_Y[selected_output_index]\n",
    "selected_gp_X = list_total_X[selected_output_index]\n",
    "selected_gp_pred_mean = list_idgp_total_output_dist[selected_output_index].loc.detach()\n",
    "selected_gp_pred_std = list_idgp_total_output_dist[selected_output_index].stddev.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traindata_testdata_fittedgp(train_X=selected_train_X, train_Y=selected_train_Y, test_X=selected_test_X, test_Y=selected_test_Y, gp_X=selected_gp_X, gp_pred_mean=selected_gp_pred_mean, gp_pred_std=selected_gp_pred_std, inducing_points_X=selected_model.variational_strategy.inducing_points.detach(), n_inducing_C=idgp_n_inducing_C) # NOTE: input is C not X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPLVM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
