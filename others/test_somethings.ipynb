{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from linear_operator.operators import KroneckerProductLinearOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "index_dim = 1\n",
    "n_C = 50\n",
    "n_X = 20\n",
    "noise_scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n",
      "torch.Size([50, 50])\n"
     ]
    }
   ],
   "source": [
    "default_kernel_parameters = {'X_raw_outputscale': [0.0], 'X_raw_lengthscale': [[0.1 for _ in range(latent_dim)]],\n",
    "                            'C_raw_outputscale': [0.9], 'C_raw_lengthscale': [[0.1 for _ in range(index_dim)]]}\n",
    "C = Tensor(np.linspace(-10, 10, n_C))\n",
    "X = Tensor(np.random.multivariate_normal([0 for _ in range(latent_dim)], np.eye(latent_dim), (n_X,)))\n",
    "\n",
    "covar_module_X = ScaleKernel(RBFKernel(ard_num_dims=latent_dim))\n",
    "covar_module_C = ScaleKernel(RBFKernel(ard_num_dims=index_dim))\n",
    "\n",
    "covar_module_X.raw_outputscale.data = Tensor(default_kernel_parameters['X_raw_outputscale'])\n",
    "covar_module_X.base_kernel.raw_lengthscale.data = Tensor(default_kernel_parameters['X_raw_lengthscale'])\n",
    "covar_module_C.raw_outputscale.data = Tensor(default_kernel_parameters['C_raw_outputscale'])\n",
    "covar_module_C.base_kernel.raw_lengthscale.data = Tensor(default_kernel_parameters['C_raw_lengthscale'])\n",
    "\n",
    "covar_X = covar_module_X(X)\n",
    "print(covar_X.shape)\n",
    "covar_C = covar_module_C(C)\n",
    "print(covar_C.shape)\n",
    "# K + sigma^2 * I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_final = KroneckerProductLinearOperator(covar_X, covar_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expected shape of the kernel was torch.Size([20, 20]), but got torch.Size([1, 20, 20]). This is likely a bug in GPyTorch.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jiangxiaoyu/Desktop/All Projects/GPLVM_project_code/others/test_somethings.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jiangxiaoyu/Desktop/All%20Projects/GPLVM_project_code/others/test_somethings.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m covar_final\u001b[39m.\u001b[39;49mto_dense()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_in_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m _add_to_cache(\u001b[39mself\u001b[39m, cache_name, method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m _get_from_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2601\u001b[0m, in \u001b[0;36mLinearOperator.to_dense\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2599\u001b[0m     eye \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(num_cols, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2600\u001b[0m     eye \u001b[39m=\u001b[39m eye\u001b[39m.\u001b[39mexpand(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_shape, num_cols, num_cols)\n\u001b[0;32m-> 2601\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatmul(eye)\n\u001b[1;32m   2602\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:1831\u001b[0m, in \u001b[0;36mLinearOperator.matmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1827\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmatmul_linear_operator\u001b[39;00m \u001b[39mimport\u001b[39;00m MatmulLinearOperator\n\u001b[1;32m   1829\u001b[0m     \u001b[39mreturn\u001b[39;00m MatmulLinearOperator(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m-> 1831\u001b[0m \u001b[39mreturn\u001b[39;00m Matmul\u001b[39m.\u001b[39mapply(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentation_tree(), other, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2064\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepresentation_tree\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[1;32m   2055\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[39m    Returns a\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[39m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2062\u001b[0m \u001b[39m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m LinearOperatorRepresentationTree(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/linear_operator/operators/linear_operator_representation_tree.py:15\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[0;34m(self, linear_op)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(linear_op\u001b[39m.\u001b[39m_args, linear_op\u001b[39m.\u001b[39m_differentiable_kwargs\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(arg, \u001b[39m\"\u001b[39m\u001b[39mrepresentation\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(arg\u001b[39m.\u001b[39mrepresentation):  \u001b[39m# Is it a lazy tensor?\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         representation_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(arg\u001b[39m.\u001b[39;49mrepresentation())\n\u001b[1;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren\u001b[39m.\u001b[39mappend((\u001b[39mslice\u001b[39m(counter, counter \u001b[39m+\u001b[39m representation_size, \u001b[39mNone\u001b[39;00m), arg\u001b[39m.\u001b[39mrepresentation_tree()))\n\u001b[1;32m     17\u001b[0m         counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m representation_size\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:397\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrepresentation()\n\u001b[1;32m    394\u001b[0m \u001b[39m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# representation\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_kernel()\u001b[39m.\u001b[39mrepresentation()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_in_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m _add_to_cache(\u001b[39mself\u001b[39m, cache_name, method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m _get_from_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(method)\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_grad_enabled):\n\u001b[0;32m---> 25\u001b[0m         output \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GPLVM/lib/python3.9/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:367\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m settings\u001b[39m.\u001b[39mdebug\u001b[39m.\u001b[39mon():\n\u001b[1;32m    366\u001b[0m     \u001b[39mif\u001b[39;00m res\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape:\n\u001b[0;32m--> 367\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    368\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe expected shape of the kernel was \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mres\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThis is likely a bug in GPyTorch.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m         )\n\u001b[1;32m    372\u001b[0m \u001b[39mreturn\u001b[39;00m to_linear_operator(res)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expected shape of the kernel was torch.Size([20, 20]), but got torch.Size([1, 20, 20]). This is likely a bug in GPyTorch."
     ]
    }
   ],
   "source": [
    "covar_final.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "900912d398d5ee9550764b57e1f594cb79a91891ca87d5ef35ee62b20f4dd5b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.17 ('GPLVM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
